{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "246b18e4",
   "metadata": {},
   "source": [
    "# SMS Spam Detection Project\n",
    "\n",
    "## 1. Project Overview\n",
    "This notebook builds an end-to-end SMS spam classifier using Natural Language Processing (NLP) techniques.\n",
    "The objective is to classify SMS messages as either 'Ham' (legitimate) or 'Spam' (unsolicited).\n",
    "\n",
    "**Workflow:**\n",
    "1. **Data Ingestion**: Loading the dataset.\n",
    "2. **Data Preprocessing**: Cleaning, removal of unused columns, and target encoding.\n",
    "3. **Exploratory Data Analysis (EDA)**: analyzing data distribution and structural features.\n",
    "4. **Feature Engineering**: Creating new features like character, word, and sentence counts.\n",
    "5. **Text Preprocessing**: Tokenization, stopword removal, and stemming.\n",
    "6. **Visual Analysis**: Word clouds and frequency distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c415139f",
   "metadata": {},
   "source": [
    "## 2. Configuration & Setup\n",
    "We begin by importing the necessary libraries for data manipulation and mathematical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b824cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Data Science Utilities\n",
    "import pandas as pd  # Data manipulation\n",
    "import numpy as np   # Numerical operations\n",
    "\n",
    "# Encoder for converting text labels (ham/spam) into numbers (0/1)\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12286a2",
   "metadata": {},
   "source": [
    "## 3. Data Ingestion\n",
    "Loading the SMS Spam Collection dataset from the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a60f8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file\n",
    "# 'ISO-8859-1' encoding is used to handle special characters often found in SMS data\n",
    "df = pd.read_csv('spam.csv', encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43266fd3",
   "metadata": {},
   "source": [
    "## 4. Data Preprocessing\n",
    "We clean the dataset by removing unnecessary columns generated during import and standardizing the column names for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe61920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 'Unnamed' columns which often contain parsing errors or empty data\n",
    "df.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], inplace=True)\n",
    "\n",
    "# Renaming for standard interpretation:\n",
    "# 'v1' -> 'target' (the label)\n",
    "# 'v2' -> 'text' (the message content)\n",
    "df.rename(columns={'v1': 'target', 'v2': 'text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c3be80",
   "metadata": {},
   "source": [
    "### 4.1 Label Encoding\n",
    "Converting categorical targets into numerical format for the machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bf1214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Transform target labels: 'ham' becomes 0, 'spam' becomes 1\n",
    "df['target'] = encoder.fit_transform(df['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33892f7e",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis (EDA)\n",
    "\n",
    "### 5.1 Data Quality Check\n",
    "Ensuring data integrity by checking for null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ec4a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in each column\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80184300",
   "metadata": {},
   "source": [
    "### 5.2 Duplicate Handling\n",
    "Identifying and removing duplicate text messages to prevent bias in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7987da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of duplicate rows\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8296a71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows, keeping the first occurrence\n",
    "df.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378fd362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the shape of the dataset after duplicate removal\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f8ef85",
   "metadata": {},
   "source": [
    "### 5.3 Target Distribution Analysis\n",
    "Checking the balance between Spam (1) and Ham (0) messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d86a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get value counts for each class\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12240971",
   "metadata": {},
   "source": [
    "#### Visualizing Class Imbalance\n",
    "A pie chart to visualize the proportion of spam vs legitimate messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc0599a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a pie chart\n",
    "plt.pie(df['target'].value_counts(), labels=['ham', 'spam'], autopct='%0.2f')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e96238c",
   "metadata": {},
   "source": [
    "### 5.4 NLP Library Setup\n",
    "Installing and importing NLTK (Natural Language Toolkit) for text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec380ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install NLTK (if not already installed)\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23be5ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140538e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data packages\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4bdd60",
   "metadata": {},
   "source": [
    "## 6. Feature Engineering\n",
    "We extract new features from the raw text to help the model distinguish between spam and ham.\n",
    "\n",
    "### 6.1 Character Count\n",
    "Calculating the total length (number of characters) of each message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14fdac84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'num_characters' column\n",
    "df['num_characters'] = df['text'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8359f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the dataframe with the new feature\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deae175",
   "metadata": {},
   "source": [
    "### 6.2 Word Count\n",
    "Calculating the number of words in each message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a83d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate word count using NLTK word_tokenize\n",
    "df['num_words'] = df['text'].apply(lambda x: len(nltk.word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec0ead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1963d891",
   "metadata": {},
   "source": [
    "### 6.3 Sentence Count\n",
    "Calculating the number of sentences in each message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9b5fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentence count using NLTK sent_tokenize\n",
    "df['num_sentence'] = df['text'].apply(lambda x: len(nltk.sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c07edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352491d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import seaborn for statistical data visualization\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13c06a7",
   "metadata": {},
   "source": [
    "### 6.4 Visualizing Feature Distributions\n",
    "Comparing the character count distribution for Spam vs Ham messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a53522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot separate histograms for Ham (target=0) and Spam (target=1)\n",
    "sns.histplot(df[df['target'] == 0]['num_characters'])\n",
    "sns.histplot(df[df['target'] == 1]['num_characters'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d82cbcf",
   "metadata": {},
   "source": [
    "### 6.5 Pairwise Relationships\n",
    "Visualizing relationships between all numerical features (characters, words, sentences) to identify separation separation patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04d0535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pairplot colored by target class\n",
    "sns.pairplot(df, hue='target')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426a1a03",
   "metadata": {},
   "source": [
    "### 6.6 Correlation Matrix\n",
    "Examining the correlation between the numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03096a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display heatmap of correlations\n",
    "sns.heatmap(df.select_dtypes(include='number').corr(), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8755a926",
   "metadata": {},
   "source": [
    "## 7. Text Preprocessing Pipeline\n",
    "Preparing the text data for modeling by applying a transformation pipeline:\n",
    "1.  **Lowercasing**: Converting to lowercase.\n",
    "2.  **Tokenization**: Splitting text into words.\n",
    "3.  **Special Character Removal**: Removing non-alphanumeric characters.\n",
    "4.  **Stopword Removal**: Removing common words (is, the, of) that add little semantic meaning.\n",
    "5.  **Stemming**: Reducing words to their root form (e.g., 'dancing' -> 'danc')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8457bdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stopwords list and PortStemmer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9279aa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2233ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(text):\n",
    "    # 1. Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # 2. Tokenize\n",
    "    text = nltk.word_tokenize(text)\n",
    "    \n",
    "    # 3. Keep only alphanumeric tokens\n",
    "    y = []\n",
    "    for i in text:\n",
    "        if i.isalnum():\n",
    "            y.append(i)\n",
    "\n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "\n",
    "    # 4. Remove stopwords and punctuation\n",
    "    for i in text:\n",
    "        if i not in stopwords.words('english') and i not in string.punctuation:\n",
    "            y.append(i)\n",
    "\n",
    "    text = y[:]\n",
    "    y.clear()\n",
    "\n",
    "    # 5. Apply Stemming\n",
    "    for i in text:\n",
    "        y.append(ps.stem(i))\n",
    "    \n",
    "    return \" \".join(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7680c28a",
   "metadata": {},
   "source": [
    "### 7.1 Applying Transformation\n",
    "Applying the preprocessing function to the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4137f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'transformed_text' column with preprocessed data\n",
    "# Note: User code renamed this from transform_text in previous version\n",
    "if 'transform_text' in df.columns:\n",
    "    df.drop(columns=['transform_text'], inplace=True)\n",
    "    \n",
    "df[\"transformed_text\"] = df['text'].apply(transform_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09425f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the final processed dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc51172e",
   "metadata": {},
   "source": [
    "## 8. Visual Analysis\n",
    "Using WordClouds to visualize the most common words in both Spam and Ham messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c791f8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# Initialize WordCloud object\n",
    "wc = WordCloud(width=500, height=500, min_font_size=10, background_color='white')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9371132a",
   "metadata": {},
   "source": [
    "### 8.1 Spam Word Cloud\n",
    "Visualizing the most frequent words in Spam messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ad9b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate WordCloud for Spam (target=1)\n",
    "spam_wc = wc.generate(df[df['target'] == 1]['transformed_text'].str.cat(sep=\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80d0ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Spam WordCloud\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.imshow(spam_wc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc4e64d",
   "metadata": {},
   "source": [
    "### 8.2 Ham Word Cloud\n",
    "Visualizing the most frequent words in Ham (legitimate) messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5dbd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate WordCloud for Ham (target=0)\n",
    "ham_wc = wc.generate(df[df['target'] == 0]['transformed_text'].str.cat(sep=\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5116a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Ham WordCloud\n",
    "plt.figure(figsize=(15,6))\n",
    "plt.imshow(ham_wc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53f87d0",
   "metadata": {},
   "source": [
    "## 9. Top Frequent Words Analysis\n",
    "Identifying the top 30 most recurring words in each category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0d1152",
   "metadata": {},
   "source": [
    "### 9.1 Top 30 Spam Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43af5e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all words from Spam messages\n",
    "spam_corpus = []\n",
    "for messages in df[df['target'] == 1]['transformed_text'].tolist():\n",
    "    for word in messages.split():\n",
    "        spam_corpus.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac67db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print total number of words in spam corpus\n",
    "len(spam_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1e7bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Create a bar plot of the top 30 most common words in Spam\n",
    "sns.barplot(x=pd.DataFrame(Counter(spam_corpus).most_common(30))[0], \n",
    "            y=pd.DataFrame(Counter(spam_corpus).most_common(30))[1])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f128d3d",
   "metadata": {},
   "source": [
    "### 9.2 Top 30 Ham Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216e5bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all words from Ham messages\n",
    "ham_corpus = []\n",
    "for messages in df[df['target'] == 0]['transformed_text'].tolist():\n",
    "    for word in messages.split():\n",
    "        ham_corpus.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629aadf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bar plot of the top 30 most common words in Ham\n",
    "sns.barplot(x=pd.DataFrame(Counter(ham_corpus).most_common(30))[0], \n",
    "            y=pd.DataFrame(Counter(ham_corpus).most_common(30))[1])\n",
    "plt.xticks(rotation='vertical')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
